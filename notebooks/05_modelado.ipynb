{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from optimus import Optimus\n",
    "op = Optimus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "# Data from http://rpubs.com/rhuebner/HRCodebook-13\n",
    "df = op.read.csv(\"data/hr-data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the data\n",
    "df.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data cleaning\n",
    "from pyspark.sql.functions import when, count, col, isnull\n",
    "\n",
    "\n",
    "integer_cols = [\"MarriedID\",\"MaritalStatusID\", \"EmpStatusID\", \"DeptID\", \"PerfScoreID\", \"PositionID\", \"Termd\", \"ManagerID\", \n",
    "                \"EmpSatisfaction\", \"SpecialProjectsCount\", \"DaysLateLast30\", \"GenderID\"]\n",
    "float_cols = [\"PayRate\", \"EngagementSurvey\"]\n",
    "\n",
    "for col_name in integer_cols:\n",
    "    df = df.withColumn(col_name, col(col_name).cast('int'))\n",
    "    \n",
    "for col_name in float_cols:\n",
    "    df = df.withColumn(col_name, col(col_name).cast('float'))\n",
    "    \n",
    "df = df.dropna(how=\"all\")\n",
    "df = df.cols.years_between(\"DOB\", date_format=\"mm/dd/yy\",output_cols=\"Age\")\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [10, 20, 30, 40, 50, 60, float(\"inf\")]\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"Age\", outputCol=\"Age_bucket\")\n",
    "df = bucketizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de datos faltantes por columna\n",
    "from pyspark.sql.functions import when, count, col, isnull\n",
    "\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how=\"any\", subset=[\"ManagerID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de datos faltantes por columna\n",
    "from pyspark.sql.functions import when, count, col, isnull\n",
    "\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus.ml import feature as fe\n",
    "df = fe.vector_assembler(df,input_cols=['MaritalStatusID', 'GenderID', 'DeptID', 'PayRate', \"ManagerID\", \n",
    "                                   'EmpSatisfaction', 'Age_bucket'], output_col=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=5, featuresCol=\"features\",\n",
    "                         outputCol=\"selected_features\", labelCol=\"Termd\")\n",
    "\n",
    "result = selector.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"Termd\", featuresCol=\"selected_features\", maxIter=10)\n",
    "model = gbt.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Termd\", \"selected_features\").table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Termd\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de datos\n",
    "(training_data, test_data) = result.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"Termd\", featuresCol=\"selected_features\", maxIter=10)\n",
    "model = gbt.fit(training_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Termd\", \"selected_features\").table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Termd\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.featureImportances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
